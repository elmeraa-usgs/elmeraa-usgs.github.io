{
  "hash": "b70615e95261ea134bd156fdc7f31d76",
  "result": {
    "markdown": "---\ntitle: \"Text wrangling and analysis\"\ndescription: |\n   Counts and visualizations for the most frequently used words in \"The Origin of Species\" by Charles Darwin, 6th Edition.\nauthor:\n  - name: Elmera Azadpour\ndate: 2021-02-23\nformat: html\ncategories: [code, analysis, text, word cloud]\nimage: \"wordcloud.jpg\"\n---\n\n\n## Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tidytext)\nlibrary(textdata)\nlibrary(pdftools)\nlibrary(ggwordcloud)\n```\n:::\n\n\n### To access data, html and Rmd/qmd files:\n\n-   the Origin of Species pdf can be found in the *data* folder of [this repo](https://github.com/elmeraa/244-Assignment3); the Rmd and html files, denoted as *a3-task3*, can be found in the *src* folder\n-   the qmd format can be found [here](https://github.com/elmeraa/elmeraa.github.io/tree/main/posts/2021-02-23-text-wrangling-and-analysis)\n\n\n## Read in the Origin of Species, 6th Edition by Darwin\n\n\n::: {.cell}\n\n```{.r .cell-code}\nos_text <- pdf_text(here(\"posts\", \"2021-02-23-text-wrangling-and-analysis\", \"originofspecies6th-darwin.pdf\"))\n```\n:::\n\n\n## Text into a data frame, then wrangling with the tidyverse, break it up by chapter, and do some analyses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nos_tidy <- data.frame(os_text) %>% \n  mutate(text_full = str_split(os_text, pattern = '\\\\n')) %>% \n  unnest(text_full) %>% \n  mutate(text_full = str_trim(text_full))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nos_df <- os_tidy %>% \n  slice(-(1:184)) %>% \nmutate(chapter = case_when(\n     str_detect(text_full, \"CHAPTER\") ~ str_extract(text_full, \"CHAPTER [1-9]+\"),\n     TRUE ~ NA_character_\n   )) %>% \n  fill(chapter) %>% \n  separate(col = chapter, into = c(\"cha\", \"no\"), sep = \" \") %>% \n  mutate(chapter = as.numeric(no))\n```\n:::\n\n\n## Word count by Chapter\n\n\n::: {.cell}\n\n```{.r .cell-code}\nos_tokens <- os_df %>% \n  unnest_tokens(word, text_full) %>% \n  select(-os_text) \n\nos_tokens_clean <- os_tokens %>%\n   mutate(word = str_replace(word, \"[0-9-]+\", NA_character_)) %>% \n  drop_na()\n\nos_wordcount <- os_tokens_clean %>% \n  count(chapter, word) \n```\n:::\n\n\n## Remove stop words and recounting again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nos_nonstop_words <- os_tokens_clean %>% \n  anti_join(stop_words)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n```{.r .cell-code}\nnonstop_counts <- os_nonstop_words %>% \n  count(chapter, word) \n```\n:::\n\n\n## Top 5 words by chapter\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_5_words <- nonstop_counts %>% \n  group_by(chapter) %>% \n  arrange(-n) %>% \n  slice(1:5)\n\n## vizualization\nggplot( data = top_5_words,\n        aes(word,n )) +\n  geom_col(fill = \"blue\") +\n  facet_wrap(~chapter, scales = \"free\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](text-wrangling-and-analysis_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Word cloud for all text\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnonstop_counts_full <- os_nonstop_words %>% \n  count(word) \n\nos_top100_removesps <- nonstop_counts_full %>% \n  arrange(-n) %>% \n  slice(1:100)\n\nos_cloud <- ggplot(data = os_top100_removesps, aes(label = word)) +\n  geom_text_wordcloud(aes(color = n, size = n), shape = \"circle\") +\n  scale_size_area(max_size = 10) +\n  scale_color_gradient(low = \"darkseagreen\", high = \"forestgreen\") +\n  theme_minimal()\n\nos_cloud\n```\n\n::: {.cell-output-display}\n![](text-wrangling-and-analysis_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#ggsave(here(\"src\",\"originofspecies-wc-ea.png\"), width = 8, height = 5) # to save\n```\n:::\n\n\n## Sentiment analysis using \"NRC\" lexicon\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## nrc to just check out\nos_nrc <- os_nonstop_words %>% \n  inner_join(get_sentiments(\"nrc\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining, by = \"word\"\n```\n:::\n\n```{.r .cell-code}\nos_nrc_counts <- os_nrc %>% \n  count(chapter, sentiment)\n\nch_names <- list(\n  \"1\" =\"Chapter 1\",\n  \"2\" = \"Chapter 2\",\n  \"3\" = \"Chapter 3\",\n  \"4\" = \"Chapter 4\",\n  \"5\" = \"Chapter 5\",\n  \"6\" = \"Chapter 6\",\n  \"7\" = \"Chapter 7\",\n  \"8\" = \"Chapter 8\",\n  \"9\" = \"Chapter 9\",\n  \"10\" = \"Chapter 10\",\n  \"11\" = \"Chapter 11\",\n  \"12\" = \"Chapter 12\",\n  \"13\" = \"Chapter 13\",\n  \"14\" = \"Chapter 14\",\n  \"15\" = \"Chapter 15\"\n )\n\nch_labeller <- function(variable,value){\n  return(ch_names[value])\n}\n\nggplot(data = os_nrc_counts, aes(x = sentiment, y = n)) +\n  geom_col() +\n  facet_wrap(~chapter, labeller=ch_labeller) +\n  coord_flip() +\n  theme_minimal() +\n  labs(y = \"Word Count\", x = \"NRC sentiment\", title = \"Sentiment analysis of the Origin of Species by Charles Darwin\\nusing NRC from Saif Mohammad and Peter Turney\")\n```\n\n::: {.cell-output-display}\n![](text-wrangling-and-analysis_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n## Citations:\n\n-   NRC lexicon: Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.\n-   Origin of Species Text: https://laurieximenez.files.wordpress.com/2019/04/the-origin-of-species_charles-darwin.pdf\n",
    "supporting": [
      "text-wrangling-and-analysis_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}